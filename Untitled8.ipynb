{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rcsvphF408u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDbEVFiT5HVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mkdir images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6axGid35jIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip -uq \"/content/drive/My Drive/IDC_regular_ps50_idx5.zip\" -d \"/content/images\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoBbwjMb5uAV",
        "colab_type": "code",
        "outputId": "55da13b2-e6ad-4526-c192-92adedba29b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "from glob import glob\n",
        "import fnmatch\n",
        "import cv2\n",
        "import argparse\n",
        "import pickle\n",
        "import os\n",
        "import sklearn\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score, classification_report\n",
        "import keras\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import plot_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n",
        "from keras.layers import Dense, Dropout, Embedding, SpatialDropout1D, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\n",
        "%matplotlib inline\n",
        "import random\n",
        "\n",
        "\n",
        "from keras.preprocessing import image #for image preprocessing                  \n",
        "from tqdm import tqdm # to maintain a progress bar\n",
        "import os\n",
        "import time\n",
        "from datetime import timedelta\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 6419534045650282311\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 8368405753411106872\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 15915442457640380906\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15956161332\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 8083513183164730532\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgQK0UvJ57u0",
        "colab_type": "code",
        "outputId": "e14180d9-9a07-4937-b7bd-2b9c31e8607c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "def init_variables():\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "init_variables()\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EHqMbLs6Xtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialization of global variables\n",
        "\n",
        "n_inputs = 227 * 227 * 3 # number of input vector elements i.e. pixels per training example\n",
        "# dropout = 0.5 # Dropout, probability to keep units\n",
        "n_classes = 2 # number of classes to be classified\n",
        "weight_decay = 0.0005\n",
        "\n",
        "lowerIndex = 0\n",
        "upperIndex = 6000\n",
        "\n",
        "# input and output vector placeholders\n",
        "x = tf.placeholder(tf.float32, [None, 227,227,3])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "flag_training=tf.placeholder(tf.bool)\n",
        "# keep_ratio = tf.placeholder(tf.float32) #dropout (keep probability)\n",
        "\n",
        "\n",
        "# fully connected layer\n",
        "fc_layer = lambda x, W, b, name=None: tf.nn.bias_add(tf.matmul(x, W), b)\n",
        "\n",
        "\n",
        "# Weights parameters as devised in the original research paper\n",
        "weights = {\n",
        "    \"wc1\": tf.Variable(tf.random.truncated_normal([11,11,3, 96],     stddev=0.01), name=\"wc1\"),\n",
        "    \"wc2\": tf.Variable(tf.random.truncated_normal([5,5,96, 256],     stddev=0.01), name=\"wc2\"),\n",
        "    \"wc3\": tf.Variable(tf.random.truncated_normal([3, 3, 256, 384],    stddev=0.01), name=\"wc3\"),\n",
        "    \"wc4\": tf.Variable(tf.random.truncated_normal([3, 3, 384, 256],    stddev=0.01), name=\"wc4\"),\n",
        "    \"wc5\": tf.Variable(tf.random.truncated_normal([3, 3, 256,256],    stddev=0.01), name=\"wc5\"),\n",
        "    \"wf1\": tf.Variable(tf.random.truncated_normal([6*6*256, 4096],   stddev=0.01), name=\"wf1\"),\n",
        "    \"wf2\": tf.Variable(tf.random.truncated_normal([4096, 4096],        stddev=0.01), name=\"wf2\"),\n",
        "    \"wf3\": tf.Variable(tf.random.truncated_normal([4096, n_classes],   stddev=0.01), name=\"wf3\")\n",
        "}\n",
        "# Bias parameters as devised in the original research paper\n",
        "biases = {\n",
        "    \"bc1\": tf.Variable(tf.constant(0.0, shape=[96]),        name=\"bc1\"),\n",
        "    \"bc2\": tf.Variable(tf.constant(1.0, shape=[256]),       name=\"bc2\"),\n",
        "    \"bc3\": tf.Variable(tf.constant(0.0, shape=[384]),       name=\"bc3\"),\n",
        "    \"bc4\": tf.Variable(tf.constant(1.0, shape=[256]),       name=\"bc4\"),\n",
        "    \"bc5\": tf.Variable(tf.constant(1.0, shape=[256]),       name=\"bc5\"),\n",
        "    \"bf1\": tf.Variable(tf.constant(1.0, shape=[4096]),      name=\"bf1\"),\n",
        "    \"bf2\": tf.Variable(tf.constant(1.0, shape=[4096]),      name=\"bf2\"),\n",
        "    \"bf3\": tf.Variable(tf.constant(1.0, shape=[n_classes]), name=\"bf3\")\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtiQW3Gv6a3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _random_flip_leftright(batch):\n",
        "    for i in range(len(batch)):\n",
        "        if bool(random.getrandbits(1)):\n",
        "            batch[i] = np.fliplr(batch[i])\n",
        "    return batch\n",
        "\n",
        "def data_augmentation(batch):\n",
        "    batch = _random_flip_leftright(batch)\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcsiYLoz6mTK",
        "colab_type": "code",
        "outputId": "32ecc53a-e9a1-4be6-b41e-aee172ded502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# source: https://www.kaggle.com/paultimothymooney/predicting-idc-in-breast-cancer-histology-images/notebook\n",
        "imagePatches = glob('/content/images/**/*.png', recursive=True) # search pathname/folder to find .png files recursively then parse files\n",
        "for filename in imagePatches[0:10]:\n",
        "    print(filename)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/images/13916/1/13916_idx5_x301_y1451_class1.png\n",
            "/content/images/13916/1/13916_idx5_x1051_y951_class1.png\n",
            "/content/images/13916/1/13916_idx5_x1801_y701_class1.png\n",
            "/content/images/13916/1/13916_idx5_x1601_y551_class1.png\n",
            "/content/images/13916/1/13916_idx5_x1601_y801_class1.png\n",
            "/content/images/13916/1/13916_idx5_x601_y1251_class1.png\n",
            "/content/images/13916/1/13916_idx5_x1851_y651_class1.png\n",
            "/content/images/13916/1/13916_idx5_x951_y1451_class1.png\n",
            "/content/images/13916/1/13916_idx5_x1051_y1301_class1.png\n",
            "/content/images/13916/1/13916_idx5_x1651_y551_class1.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzdVHl5T6sxr",
        "colab_type": "code",
        "outputId": "98189da1-7282-4b0b-9ac3-20f75cc459eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "patternZero = '*class0.png'\n",
        "patternOne = '*class1.png'\n",
        "classZero = fnmatch.filter(imagePatches, patternZero) #filename pattern matching returns subset of list of names\n",
        "classOne = fnmatch.filter(imagePatches, patternOne)\n",
        "print(\"IDC(-)\\n\\n\",classZero[0:5],'\\n')\n",
        "print(\"IDC(+)\\n\\n\",classOne[0:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IDC(-)\n",
            "\n",
            " ['/content/images/13916/0/13916_idx5_x1051_y1751_class0.png', '/content/images/13916/0/13916_idx5_x1101_y351_class0.png', '/content/images/13916/0/13916_idx5_x1251_y901_class0.png', '/content/images/13916/0/13916_idx5_x1451_y1551_class0.png', '/content/images/13916/0/13916_idx5_x1201_y751_class0.png'] \n",
            "\n",
            "IDC(+)\n",
            "\n",
            " ['/content/images/13916/1/13916_idx5_x301_y1451_class1.png', '/content/images/13916/1/13916_idx5_x1051_y951_class1.png', '/content/images/13916/1/13916_idx5_x1801_y701_class1.png', '/content/images/13916/1/13916_idx5_x1601_y551_class1.png', '/content/images/13916/1/13916_idx5_x1601_y801_class1.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3GzsR2q7CAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def path_to_tensor(img_path):\n",
        "    img = image.load_img(img_path, target_size=(227,227))\n",
        "    x = image.img_to_array(img)\n",
        "    return np.expand_dims(x, axis=0)\n",
        "\n",
        "def paths_to_tensor(img_paths):\n",
        "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
        "    return np.vstack(list_of_tensors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig1HPh8o7JOp",
        "colab_type": "code",
        "outputId": "25f6869d-51d0-4022-dcb2-4ee28f001559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "imageIndex = imagePatches[lowerIndex:upperIndex]\n",
        "x_ = paths_to_tensor(imageIndex).astype('float32')/255"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6000/6000 [00:07<00:00, 756.88it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTjLS0CI7NBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def proc_images(lowerIndex,upperIndex):\n",
        "    \"\"\"\n",
        "    Returns two arrays: \n",
        "        x is an array of resized images\n",
        "        y is an array of labels\n",
        "    \"\"\" \n",
        "    x = []\n",
        "    y = []\n",
        "    for img in imagePatches[lowerIndex:upperIndex]:\n",
        "        x.append(x_[lowerIndex:upperIndex])\n",
        "        if img in classZero:\n",
        "            y.append(0)\n",
        "        elif img in classOne:\n",
        "            y.append(1)\n",
        "        else:\n",
        "            return\n",
        "    return x,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3KwY3Jm7Sho",
        "colab_type": "code",
        "outputId": "97e3e271-6d12-4334-b37b-b87b3af428ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%time\n",
        "# n = len(imagePatches)\n",
        "X,Y = proc_images(0,10000)\n",
        "df = pd.DataFrame()\n",
        "df[\"images\"]=X\n",
        "df[\"labels\"]=Y\n",
        "X2=df[\"images\"]\n",
        "Y2=df[\"labels\"]\n",
        "X2=np.array(X2)\n",
        "imgs0=[]\n",
        "imgs1=[]\n",
        "imgs0 = X2[Y2==0] # (0 = Background, 1 = Number Plate)\n",
        "imgs1 = X2[Y2==1] "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
            "Wall time: 7.63 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2eoizEE9veo",
        "colab_type": "code",
        "outputId": "6a2dc2e4-dc72-430f-f860-7bc4811f40db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "Y = to_categorical(Y)\n",
        "print(Y)\n",
        "print(Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "(10000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA6biYpS9x8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, X_test, Y_train, Y_test) = train_test_split(X,Y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiYRnSGK90aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def alex_net(img, weights, biases, flag_training):\n",
        "\n",
        "    # reshape the input image vector to 227 x 227 x 3 dimensions\n",
        "    img = tf.reshape(img, [-1, 227, 227, 3])\n",
        "\n",
        "    # 1st convolutional layer\n",
        "    conv1 = tf.nn.conv2d(img, weights[\"wc1\"], strides=[1, 4, 4, 1], padding=\"SAME\", name=\"conv1\")\n",
        "    conv1 = tf.nn.bias_add(conv1, biases[\"bc1\"])\n",
        "    conv1 = tf.nn.relu(tf.layers.batch_normalization(conv1,training=flag_training))\n",
        "    conv1 = tf.nn.local_response_normalization(conv1, depth_radius=5.0, bias=2.0, alpha=1e-4, beta=0.75)\n",
        "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
        "\n",
        "    # 2nd convolutional layer\n",
        "    conv2 = tf.nn.conv2d(conv1, weights[\"wc2\"], strides=[1, 1, 1, 1], padding=\"SAME\", name=\"conv2\")\n",
        "    conv2 = tf.nn.bias_add(conv2, biases[\"bc2\"])\n",
        "    conv2 = tf.nn.relu(tf.layers.batch_normalization(conv2,training=flag_training))\n",
        "    conv2 = tf.nn.local_response_normalization(conv2, depth_radius=5.0, bias=2.0, alpha=1e-4, beta=0.75)\n",
        "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
        "\n",
        "\n",
        "    # 3rd convolutional layer\n",
        "    conv3 = tf.nn.conv2d(conv2, weights[\"wc3\"], strides=[1, 1, 1, 1], padding=\"SAME\", name=\"conv3\")\n",
        "    conv3 = tf.nn.bias_add(conv3, biases[\"bc3\"])\n",
        "    conv3 = tf.nn.relu(tf.layers.batch_normalization(conv3,training=flag_training))\n",
        "\n",
        "    # 4th convolutional layer\n",
        "    conv4 = tf.nn.conv2d(conv3, weights[\"wc4\"], strides=[1, 1, 1, 1], padding=\"SAME\", name=\"conv4\")\n",
        "    conv4 = tf.nn.bias_add(conv4, biases[\"bc4\"])\n",
        "    conv4 = tf.nn.relu(tf.layers.batch_normalization(conv4,training=flag_training))\n",
        "\n",
        "    # 5th convolutional layer\n",
        "    conv5 = tf.nn.conv2d(conv4, weights[\"wc5\"], strides=[1, 1, 1, 1], padding=\"SAME\", name=\"conv5\")\n",
        "    conv5 = tf.nn.bias_add(conv5, biases[\"bc5\"])\n",
        "    conv5 = tf.nn.relu(tf.layers.batch_normalization(conv5,training=flag_training))\n",
        "    conv5 = tf.nn.max_pool(conv5, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
        "\n",
        "    # stretching out the 5th convolutional layer into a long vector\n",
        "    shape = [-1, weights['wf1'].get_shape().as_list()[0]]\n",
        "    flatten = tf.reshape(conv5, shape)\n",
        "\n",
        "    # 1st fully connected layer\n",
        "    fc1 = fc_layer(flatten, weights[\"wf1\"], biases[\"bf1\"], name=\"fc1\")    \n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    fc1 = tf.nn.dropout(fc1, keep_prob=0.5)\n",
        "\n",
        "    # 2nd fully connected layer\n",
        "    fc2 = fc_layer(fc1, weights[\"wf2\"], biases[\"bf2\"], name=\"fc2\")   \n",
        "    fc2 = tf.nn.relu(fc2)\n",
        "    fc2 = tf.nn.dropout(fc2, keep_prob=0.5)\n",
        "\n",
        "    # 3rd fully connected layer\n",
        "    fc3 = fc_layer(fc2, weights[\"wf3\"], biases[\"bf3\"], name=\"fc3\")\n",
        "    fc3 = tf.nn.softmax(fc3)\n",
        "\n",
        "    # Return the complete AlexNet model\n",
        "    return fc3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G7SZ2l196if",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def performance(x, w, b, f):\n",
        "    \n",
        "    update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    logits_ph = alex_net(x, w, b, f)\n",
        "    cross_entropy_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits_ph), name='cross_entropy_op')\n",
        "    l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])    \n",
        "    cross_entropy_op = cross_entropy_op + (l2_loss * weight_decay)\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cross_entropy_op)\n",
        "    train_op = tf.group([train_op, update_ops])\n",
        "    return logits_ph, cross_entropy_op, train_op\n",
        "    \n",
        "# init_variables()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sROEtMFG99iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = data_augmentation(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-fort8w-AWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(filename):\n",
        "    \n",
        "    saver = tf.train.Saver(tf.global_variables(), max_to_keep = 3)\n",
        "    save_dir = filename\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    print('Directory {} has been created'.format(save_dir))\n",
        "    return saver, save_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB8GnKX4-CXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs):\n",
        "    \n",
        "#     init_variables()\n",
        "    ntrain = int(len(X_train))\n",
        "    logits_ph, cross_entropy_op, train_op = performance(x, weights, biases, flag_training)\n",
        "    saver, save_dir = save_checkpoint('Saver')\n",
        "    correct_op = tf.equal(tf.argmax(logits_ph, 1), tf.argmax(y, 1))\n",
        "    accuracy_op = tf.reduce_mean(tf.cast(correct_op, tf.float32), name='accuracy_op')\n",
        "    batch_size = 128\n",
        "    n_classes = 2\n",
        "    display_step = 1\n",
        "\n",
        "  \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        for epoch in range(epochs):\n",
        "            loss_test = 0.\n",
        "            loss_train = 0.\n",
        "            start_time = time.time()\n",
        "            num_batch = int(ntrain/batch_size) + 1\n",
        "            for i in range(num_batch):\n",
        "                batch_xs = X_train[i*batch_size:min((i+1)*batch_size,ntrain)]\n",
        "                batch_ys = Y_train[i*batch_size:min((i+1)*batch_size,ntrain)]\n",
        "                # batch_xs = data_augmentation(batch_xs)\n",
        "\n",
        "                sess.run(train_op, feed_dict={x: batch_xs, y: batch_ys, flag_training:True})\n",
        "                loss_train += sess.run(cross_entropy_op, feed_dict={x: batch_xs, y: batch_ys, flag_training:True})/num_batch\n",
        "                train_acc = sess.run(accuracy_op, feed_dict={x: batch_xs, y: batch_ys, flag_training:True})\n",
        "   \n",
        "            if epoch % display_step == 0 :\n",
        "                \n",
        "                loss_test += sess.run(cross_entropy_op, feed_dict={x: X_test, y: Y_test, flag_training:False})/num_batch\n",
        "                test_acc = sess.run(accuracy_op, feed_dict={x: X_test, y: Y_test, flag_training:False})\n",
        "\n",
        "            print(\"Epoch {} completed out of {}: loss_train-{:.6f} loss_test-{:.6f} train_acc-{:.3f} test_acc-{:.3f}\"\n",
        "                  .format(epoch+1, epochs, loss_train, loss_test,train_acc, test_acc))\n",
        "\n",
        "\n",
        "            ckpt_name = os.path.join(save_dir, 'epoch')\n",
        "            z = saver.save(sess, save_path=ckpt_name, global_step=epoch+1)\n",
        "            print('checkpoint saved in {}'.format(z))\n",
        "            \n",
        "            # Ending time.\n",
        "            end_time = time.time()\n",
        "            # Difference between start and end-times.\n",
        "            time_dif = end_time - start_time\n",
        "            # Print the time-usage.\n",
        "            print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
        "\n",
        "    print('\\nOptimization Finished')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkSnjNHH-F7o",
        "colab_type": "code",
        "outputId": "9c9a9a98-5fca-4303-86fb-6879522f6a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "train(300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-16-f09c2277c4b6>:9: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-16-f09c2277c4b6>:44: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Directory Saver has been created\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}